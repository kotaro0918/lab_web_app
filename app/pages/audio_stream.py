"""
Streamlit UI for realâ€‘time voice chat with Gemini LiveÂ API (WebRTC version)
===========================================================================
â€¢ This version runs entirely in the browser using streamlit-webrtc.
â€¢ Click the "START" button in the component below to begin.
â€¢ The assistantâ€™s responses will be spoken back and simultaneously shown
  as text.
â€¢ Click "STOP" to end the session.

> Ensure GOOGLE_API_KEY (or GEMINI_API_KEY) is set in credential/.env.
"""

from __future__ import annotations
import asyncio
import logging
import threading  # â˜… è¿½åŠ 
from queue import Queue, Empty
import time  # â˜… è¿½åŠ 

import av
import numpy as np
import streamlit as st
from google import genai
from streamlit_webrtc import AudioProcessorBase, WebRtcMode, webrtc_streamer

# --- åŸºæœ¬è¨­å®š ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
MODEL = "gemini-2.5-flash-exp-native-audio-thinking-dialog"
CONFIG = {
    "response_modalities": ["AUDIO"],
    "system_instruction": "You are a Japanese helpful assistant and answer in a friendly tone.answer in Japanese. and shortly.",
}

try:
    client = genai.Client()
except Exception as e:
    st.error(f"Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()


# --- WebRTC ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ—ãƒ­ã‚»ãƒƒã‚µ ---
class GeminiAudioProcessor(AudioProcessorBase):
    # â˜… ä¿®æ­£: __init__ã‹ã‚‰text_queueã‚’å‰Šé™¤
    def __init__(self):
        # â˜… ä¿®æ­£: text_queueã¯å¾Œã‹ã‚‰è¨­å®šã•ã‚Œã‚‹ã®ã§Noneã§åˆæœŸåŒ–
        self.text_queue: Queue | None = None
        self.session: genai.aio.LiveSession | None = None
        self.in_queue: asyncio.Queue[bytes] = asyncio.Queue()
        self.out_queue: asyncio.Queue[bytes | None] = asyncio.Queue()
        self.is_playing = asyncio.Event()
        self.is_speaking = False

        # â˜…â˜…â˜… ä¿®æ­£: ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã¨ãã‚Œã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— â˜…â˜…â˜…
        self.loop = asyncio.new_event_loop()
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        self.task: asyncio.Task | None = None

    def _run_loop(self):
        """ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã™ã‚‹"""
        asyncio.set_event_loop(self.loop)
        self.loop.run_forever()

    def start(self):
        """éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã™ã‚‹"""
        if self.task is None or self.task.done():
            # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªæ–¹æ³•ã§ã‚¿ã‚¹ã‚¯ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã™ã‚‹
            asyncio.run_coroutine_threadsafe(self._main_loop(), self.loop)
            logger.info("Audio processor task started.")

    def stop(self):
        """éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’åœæ­¢ã™ã‚‹"""
        if self.task:
            self.task.cancel()
            try:
                self.loop.run_until_complete(self.task)
            except asyncio.CancelledError:
                pass
            finally:
                self.task = None
        self.loop.close()
        logger.info("Audio processor task stopped.")

    async def _main_loop(self):
        """Geminiã¨ã®é€šä¿¡ã¨ãƒ‡ãƒ¼ã‚¿ä¸­ç¶™ã‚’è¡Œã†ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        try:
            async with client.aio.live.connect(model=MODEL, config=CONFIG) as session:
                self.session = session
                logger.info("Gemini session connected.")
                # é€ä¿¡ã‚¿ã‚¹ã‚¯ã¨å—ä¿¡ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œ
                await asyncio.gather(self._sender(), self._receiver())
        except Exception as e:
            logger.error(f"Error in main loop: {e}")

    async def _sender(self):
        """ãƒã‚¤ã‚¯ã‹ã‚‰ã®éŸ³å£°ã‚’Geminiã«é€ä¿¡ã™ã‚‹"""
        while True:
            try:
                chunk = await self.in_queue.get()
                # â˜…â˜…â˜… ä¿®æ­£: is_playingãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹é–“ã¯é€ä¿¡ã—ãªã„ â˜…â˜…â˜…
                if self.session and not self.is_playing.is_set():
                    await self.session.send_realtime_input(
                        audio={
                            "data": chunk,
                            "mime_type": f"audio/pcm;rate={SEND_SAMPLE_RATE}",
                        }
                    )
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Sender error: {e}")

    async def _receiver(self):
        """Geminiã‹ã‚‰ã®å¿œç­”ã‚’å—ä¿¡ã—ã€å†ç”Ÿã‚­ãƒ¥ãƒ¼ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹"""
        while True:
            try:
                if not self.session:
                    await asyncio.sleep(0.1)
                    continue

                async for resp in self.session.receive():
                    # â˜…â˜…â˜… ä¿®æ­£: å¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã‚’æŸ”è»Ÿã«å‡¦ç† â˜…â˜…â˜…
                    def process_part(part):
                        if part.audio and part.audio.data:
                            self.out_queue.put_nowait(part.audio.data)
                        if self.text_queue and part.text:
                            self.text_queue.put_nowait(part.text)

                    if hasattr(resp, "parts") and resp.parts:
                        for part in resp.parts:
                            process_part(part)
                    else:
                        # partsãŒãªã„å ´åˆã€å¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè‡ªä½“ã‚’ãƒã‚§ãƒƒã‚¯
                        process_part(resp)

                # ã‚¿ãƒ¼ãƒ³ã®çµ‚ã‚ã‚Šã«Noneã‚’å…¥ã‚Œã¦å†ç”Ÿã®åŒºåˆ‡ã‚Šã¨ã™ã‚‹
                self.out_queue.put_nowait(None)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Receiver error: {e}")

    async def recv_queued(self, frames: list[av.AudioFrame]) -> list[av.AudioFrame]:
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ãƒã‚¤ã‚¯éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹"""
        # â˜…â˜…â˜… ä¿®æ­£: is_playingãŒã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ã‚‹é–“ã¯ãƒã‚¤ã‚¯å…¥åŠ›ã‚’ç„¡è¦– â˜…â˜…â˜…
        if self.is_playing.is_set():
            return []

        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦16kHz, ãƒ¢ãƒãƒ©ãƒ«, 16bit PCMã«å¤‰æ›
        resampler = av.AudioResampler(
            format="s16", layout="mono", rate=SEND_SAMPLE_RATE
        )
        processed_frames = []
        for frame in frames:
            processed_frames.extend(resampler.resample(frame))

        if not processed_frames:
            return []  # â˜…â˜…â˜… ä¿®æ­£: Noneã§ã¯ãªãç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ â˜…â˜…â˜…

        # å¤‰æ›ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã¦å…¥åŠ›ã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹
        pcm_s16 = np.hstack([p.to_ndarray() for p in processed_frames])
        await self.in_queue.put(pcm_s16.tobytes())
        # recv_queued ã¯ä½•ã‚‚è¿”ã™å¿…è¦ãŒãªã„ã®ã§ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return []

    async def send_queued(self) -> list[av.AudioFrame]:
        """å†ç”Ÿã‚­ãƒ¥ãƒ¼ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã«é€ã‚‹"""
        if not self.is_speaking:
            # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒæ¥ã‚‹ã¾ã§å¾…ã¤
            first_frame = await self.out_queue.get()
            if first_frame is None:
                self.is_playing.clear()
                return []  # â˜… ä¿®æ­£: Noneã§ã¯ãªãç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            self.is_playing.set()
            self.is_speaking = True
            chunk = first_frame
        else:
            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                chunk = await asyncio.wait_for(self.out_queue.get(), timeout=0.1)
                if chunk is None:
                    self.is_speaking = False
                    self.is_playing.clear()
                    return []  # â˜… ä¿®æ­£: Noneã§ã¯ãªãç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            except asyncio.TimeoutError:
                return []  # â˜… ä¿®æ­£: ç„¡éŸ³ã‚’è¿”ã™å ´åˆã‚‚ç©ºã®ãƒªã‚¹ãƒˆ

        # å—ã‘å–ã£ãŸPCMãƒ‡ãƒ¼ã‚¿ã‚’av.AudioFrameã«å¤‰æ›ã—ã¦è¿”ã™
        np_frame = np.frombuffer(chunk, dtype=np.int16)
        new_frame = av.AudioFrame.from_ndarray(
            np_frame.reshape(1, -1), format="s16", layout="mono"
        )
        new_frame.sample_rate = RECEIVE_SAMPLE_RATE
        return [new_frame]  # â˜… ä¿®æ­£: ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒªã‚¹ãƒˆã«å…¥ã‚Œã¦è¿”ã™

    def on_ended(self):
        """WebRTCã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.stop()


# --- Streamlit UI ---
st.set_page_config(page_title="Gemini Voice Chat (WebRTC)", page_icon="ğŸŒ")
st.title("ğŸŒ Gemini Voice Chat Demo (WebRTC)")
st.markdown("**Start**ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãƒã‚¤ã‚¯ã®ä½¿ç”¨ã‚’è¨±å¯ã—ã€ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚")

# â˜…â˜…â˜… ä¿®æ­£: st.session_state ã®åˆæœŸåŒ–ã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…ˆé ­ã«ç§»å‹• â˜…â˜…â˜…
if "text_buffer" not in st.session_state:
    st.session_state.text_buffer = ""
if "processor_started" not in st.session_state:
    st.session_state.processor_started = False
if "audio_processor" not in st.session_state:
    st.session_state.audio_processor = None
if "webrtc_ctx" not in st.session_state:
    st.session_state.webrtc_ctx = None

# --- ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ---
text_placeholder = st.empty()
audio_placeholder = st.empty()

# â˜…â˜…â˜… ä¿®æ­£: UIã¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ†é›¢ â˜…â˜…â˜…
col1, col2 = st.columns([1, 1])
with col1:
    start_button = st.button(
        "â–¶ï¸ Start Conversation", key="start", use_container_width=True
    )
with col2:
    stop_button = st.button("â¹ï¸ Stop Conversation", key="stop", use_container_width=True)

if start_button:
    st.session_state.webrtc_ctx = webrtc_streamer(
        key="gemini-webrtc",
        mode=WebRtcMode.SENDONLY,  # â˜…â˜…â˜… ä¿®æ­£: SENDONLYãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
        audio_processor_factory=GeminiAudioProcessor,
        media_stream_constraints={"video": False, "audio": True},
        async_processing=True,
    )
    st.rerun()

if stop_button and st.session_state.webrtc_ctx:
    st.session_state.webrtc_ctx.stop()
    st.session_state.webrtc_ctx = None
    st.session_state.audio_processor = None
    st.session_state.processor_started = False
    st.session_state.text_buffer = ""
    st.rerun()


if st.session_state.webrtc_ctx and st.session_state.webrtc_ctx.state.playing:
    status_placeholder = st.success(
        "ä¼šè©±ãŒå®Ÿè¡Œä¸­ã§ã™â€¦ ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã¦ãã ã•ã„ã€‚"
    )
    if not st.session_state.processor_started:
        st.session_state.audio_processor = st.session_state.webrtc_ctx.audio_processor
        st.session_state.processor_started = True

    processor = st.session_state.audio_processor
    if processor:
        try:
            # â˜…â˜…â˜… ä¿®æ­£: éŸ³å£°å†ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜…
            audio_chunk = processor.out_queue.get_nowait()
            if audio_chunk is not None:
                # å†ç”Ÿç”¨ã®å®Œå…¨ãªéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                if "audio_buffer" not in st.session_state:
                    st.session_state.audio_buffer = b""
                st.session_state.audio_buffer += audio_chunk
            else:
                # NoneãŒæ¥ãŸã‚‰å†ç”Ÿã—ã¦ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                if "audio_buffer" in st.session_state and st.session_state.audio_buffer:
                    audio_placeholder.audio(
                        st.session_state.audio_buffer, sample_rate=RECEIVE_SAMPLE_RATE
                    )
                    st.session_state.audio_buffer = b""  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
        except Empty:
            pass  # ã‚­ãƒ¥ãƒ¼ãŒç©ºãªã‚‰ä½•ã‚‚ã—ãªã„

        try:
            # â˜…â˜…â˜… ä¿®æ­£: ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜…
            text_chunk = processor.text_queue.get_nowait()
            st.session_state.text_buffer += text_chunk
        except Empty:
            pass

    text_placeholder.markdown(
        st.session_state.text_buffer or "_ä¼šè©±ã®å±¥æ­´ã¯ã“ã“ã«è¡¨ç¤ºã•ã‚Œã¾ã™â€¦_"
    )
    st.rerun()
else:
    status_placeholder = st.info("ã€ŒStart Conversationã€ã‚’æŠ¼ã—ã¦ä¼šè©±ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
